{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning\n",
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a href=\"https://colab.research.google.com/github/marcinsawinski/UEP_KIE_DL_CODE2024/blob/main/dl03_tuning.ipynb\" target=\"_parent\">\n",
    "      <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open in Colab\"/>\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://kaggle.com/kernels/welcome?src=https://github.com/marcinsawinski/UEP_KIE_DL_CODE2024/blob/main/dl03_tuning.ipynb\">\n",
    "      <img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open in Kaggle\"/>\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://studiolab.sagemaker.aws/import/github/marcinsawinski/UEP_KIE_DL_CODE2024/blob/main/dl03_tuning.ipynb\">\n",
    "      <img src=\"https://studiolab.sagemaker.aws/studiolab.svg\" alt=\"Open in SageMaker Studio Lab\"/>\n",
    "    </a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tasks\n",
    "1. Install and connect to [WandB](https://wandb.ai) and Run a training simulation\n",
    "2. Build NN model for classification on Fashion MNIST dataset. Log training to WandB in own project.\n",
    "3. Create hyperparameter search with WandB sweep.\n",
    "4. Experiment with Optimizers (SGD, ADAM) and optimizer parameters - number of epochs, learnign rate.\n",
    "5. Experiment with network depth and number of neurons in layers\n",
    "6. Experiment with Schedulers\n",
    "7. Experiment with Dropout layers\n",
    "8. Experiment with Batch Normalization layers\n",
    "9. Try early stopping.\n",
    "10. Find optimal setup. Retrain with the optimal setup and Log training to WandB in project called DL25-FMNIST\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task1 - Install and connect to [WandB](https://wandb.ai)\n",
    "1. Run `pip install wandb -qU`in terminal or `!pip install wandb -qU` in jupyter cell\n",
    "2. Login using `wandb login` (terminal) or !wandb login' (jupyter cell). Alternatively you can run \n",
    "```\n",
    "    import wandb\n",
    "    wandb.login()\n",
    "```\n",
    "3. Update wandb.init code with your own project name e.g. student_surname_firstname. The entity can be \"uep-kie-dl25\" if you are already assigned.\n",
    "4. Run simulation and review results on the [WandB](https://wandb.ai) website.\n",
    "\n",
    "\n",
    "You will find WandB detailed example here:\n",
    "https://colab.research.google.com/github/wandb/examples/blob/master/colabs/intro/Intro_to_Weights_%26_Biases.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code for installation and login goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "import wandb\n",
    "user = \"kowalski_jan\"\n",
    "cfg = {\n",
    "    \"learning_rate\": 0.02,\n",
    "    \"architecture\": \"ANN\",\n",
    "    \"dataset\": \"dummy\",\n",
    "    \"epochs\": 5,\n",
    "}\n",
    "name = f\"{cfg['architecture']}_{cfg['dataset']}_lr{cfg['learning_rate']}_ep{cfg['epochs']}_{time.strftime('%m%d-%H%M')}\"\n",
    "project = (f\"student_{user}_dummy\")\n",
    "# Start a new wandb run to track this script.\n",
    "run = wandb.init(\n",
    "    # Set the wandb entity where your project will be logged (generally your team name).\n",
    "    entity=\"uep-kie-dl25\",\n",
    "    # Set the wandb project where this run will be logged.\n",
    "    project=project,\n",
    "    # Track hyperparameters and run metadata.\n",
    "    name=name,\n",
    "    config=cfg,\n",
    ")\n",
    "\n",
    "\n",
    "# Simulate training.\n",
    "epochs = 10\n",
    "offset = random.random() / 5\n",
    "for epoch in range(2, epochs):\n",
    "    acc = 1 - 2**-epoch - random.random() / epoch - offset\n",
    "    loss = 2**-epoch + random.random() / epoch + offset\n",
    "\n",
    "    # Log metrics to wandb.\n",
    "    run.log({\"acc\": acc, \"loss\": loss})\n",
    "\n",
    "# Finish the run and upload any remaining data.\n",
    "run.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2 - Build NN model for classification on Fashion MNIST dataset. Log training to WandB in own project.\n",
    "\n",
    "Create a basic NN model:\n",
    "- Flatten data with `nn.Flatten()` layer\n",
    "- 3 linear layers with 784, 128 and 64 inputs. outputs are 128,64 and 10 e.g. `nn.Linear(64, 10)`\n",
    "- ReLU activation `nn.ReLU()`  (no activation for output layer)\n",
    "- use cross entropy loss as criterion `nn.CrossEntropyLoss()` \n",
    "- use  SGD optimizer `optim.SGD(model.parameters(), lr=config.lr)` \n",
    "\n",
    "\n",
    "https://colab.research.google.com/github/wandb/examples/blob/master/colabs/pytorch/Organizing_Hyperparameter_Sweeps_in_PyTorch_with_W%26B.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import math\n",
    "import time\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "user = \"kowalski_jan\" # your name here \n",
    "cfg = {\n",
    "    \"learning_rate\": 0.02,\n",
    "    \"architecture\": \"ANN\",\n",
    "    \"dataset\": \"FMNIST\",\n",
    "    \"epochs\": 5,\n",
    "    \"batch_size\": 32,\n",
    "    \"lr\": 1e-3,\n",
    "}\n",
    "name = f\"{cfg['architecture']}_{cfg['dataset']}_lr{cfg['learning_rate']}_ep{cfg['epochs']}_{time.strftime('%m%d-%H%M')}\"\n",
    "project = f\"student_{user}_FMNIST\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = wandb.init(\n",
    "    # Set the wandb entity where your project will be logged (generally your team name).\n",
    "    entity=\"uep-kie-dl25\",\n",
    "    # Set the wandb project where this run will be logged.\n",
    "    project=project,\n",
    "    # Track hyperparameters and run metadata.\n",
    "    name=name,\n",
    "    config=cfg,\n",
    ")\n",
    "\n",
    "config = wandb.config\n",
    "\n",
    "# 1. Device configuration\n",
    "device = torch.device(\"mps\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 2. Transformations\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))]\n",
    ")\n",
    "\n",
    "# 3. Load the dataset\n",
    "train_dataset = datasets.FashionMNIST(\n",
    "    root=\"./data\", train=True, download=True, transform=transform\n",
    ")\n",
    "test_dataset = datasets.FashionMNIST(\n",
    "    root=\"./data\", train=False, download=True, transform=transform\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=config.batch_size * 4)\n",
    "n_steps_per_epoch = math.ceil(len(train_loader.dataset) / wandb.config.batch_size)\n",
    "\n",
    "\n",
    "# 4. Define the model\n",
    "class FashionClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FashionClassifier, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            # TODO specify your model here ...\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "\n",
    "model = FashionClassifier().to(device)\n",
    "\n",
    "# 5. Loss and optimizer\n",
    "criterion = # TODO your code here \n",
    "optimizer = # TODO your code here \n",
    "\n",
    "# 6. Training loop\n",
    "num_epochs = config.epochs\n",
    "example_ct = 0\n",
    "step_ct = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for step, (images, labels) in enumerate(train_loader):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        example_ct += len(images)\n",
    "        train_step_ct = (step + 1 + (n_steps_per_epoch * epoch)) / n_steps_per_epoch\n",
    "        log_step_ct = step + 1 + (n_steps_per_epoch * epoch)\n",
    "        metrics = {\n",
    "            \"train/train_loss\": loss,\n",
    "            \"train/epoch\": train_step_ct,\n",
    "            \"train/example_ct\": example_ct,\n",
    "        }\n",
    "        # print(log_step_ct)\n",
    "        if step + 1 < n_steps_per_epoch:\n",
    "            # Log train metrics to wandb\n",
    "            wandb.log(metrics)\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "    # 7. Evaluation\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print(f\"Test Accuracy: {100 * correct / total:.2f}%\")\n",
    "    metrics = {\n",
    "        \"test/epoch\": epoch + 1,\n",
    "        \"test/accuracy\": 100 * correct / total,\n",
    "    }\n",
    "    wandb.log(metrics)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3 - Create hyperparameter search with WandB sweep.\n",
    "Follow instructions available from WandB \n",
    "\n",
    "https://colab.research.google.com/github/wandb/examples/blob/master/colabs/pytorch/Organizing_Hyperparameter_Sweeps_in_PyTorch_with_W%26B.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment with sweeps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remaining tasks\n",
    "4. Experiment with Optimizers (SGD, ADAM) and optimizer parameters - number of epochs, learning rate.\n",
    "5. Experiment with network depth and number of neurons in layers\n",
    "6. Experiment with Schedulers\n",
    "7. Experiment with Dropout layers\n",
    "8. Experiment with Batch Normalization layers\n",
    "9. Try early stopping.\n",
    "10. Find optimal setup. Retrain with the optimal setup and Log training to WandB in project called DL25-FMNIST. use your name as job name \n",
    "\n",
    "**Optimizers**\n",
    "\n",
    "Available [Optimizers](https://pytorch.org/docs/stable/optim.html):\n",
    "```import torch.optim as optim\n",
    "optimizer = optim.SGD(model.parameters(), lr=config.lr)\n",
    "optimizer = optim.Adam(model.parameters(), lr=config.lr)\n",
    "```\n",
    "**Dropout**\n",
    "\n",
    "Dropout layers: `nn.Dropout(0.5)` - add after the ReLU activations.\n",
    "\n",
    "**Batch Normalization**\n",
    "\n",
    "Batch Normalization layers 'nn.BatchNorm1d(64)'  - add right after Linear layers and before the activation functions. \n",
    "\n",
    "\n",
    "**Scheduler**\n",
    "\n",
    "```\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "scheduler = ExponentialLR(optimizer, gamma=0.9)\n",
    "\n",
    "for epoch in range(20):\n",
    "    for input, target in dataset:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(input)\n",
    "        loss = loss_fn(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    scheduler.step()\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
